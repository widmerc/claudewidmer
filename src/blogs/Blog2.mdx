import ImageGallery from '@/app/_components/ImageGallery'
import RevealBox from "@/app/_components/RevealBox";
import React from 'react';

export const metadata = {
  title: "Schulwegsicherheit automatisiert bewerten (Teil 2)",
  excerpt: "Im zweiten Teil zeige ich, wie ich YOLOv11 mit gedrehten Bounding Boxes auf Luftbilder anwende, um FussgÃ¤ngerstreifen automatisiert zu erkennen â€“ und damit den ersten Schritt zur KI-gestÃ¼tzten Schulwegklassifikation mache.",
  coverImage: "/img/Blog2/Title.jpg",
  date: "2025-06-01T12:00:00.000Z",
  author: {
    name: "Claude Widmer",
    picture: "/img/Blog2/Title.jpg"
  },
  ogImage: {
    url: "/img/Blog2/Title.jpg"
  },
  tags: ["Masterarbeit"]
}


# ğŸ¤– Schulwegsicherheit automatisiert bewerten (Teil 2)

---

## ğŸ›°ï¸ KI auf Luftbildern: FussgÃ¤ngerstreifen automatisch erkennen

Nachdem ich im ersten Teil meine Motivation und den konzeptionellen Rahmen vorgestellt habe, geht es nun ans **Eigentliche**:

> Ich habe ein erstes Modell trainiert, das **FussgÃ¤ngerstreifen in Luftbildern automatisch erkennt** â€“ auf Basis von **YOLOv11** mit **gedrehten Bounding Boxes (OBB)**.

**Das Ziel:**
> **Automatisierte Erkennung von FussgÃ¤ngern und anderen Merkmalen auf Luftbildern**


---


## â“ Was habe ich vor â€“ was ist **Image Recognition**?

Image Recognition ist eine Art von **Artificial Intelligence (AI)**. In einfachen Worten: Sie versucht, Muster und Objekte in Bildern zu erkennen â€“ Ã¤hnlich wie das menschliche Auge, nur automatisiert und viel schneller.

### ğŸ·ï¸ Was sind die **Inputs**?
- **Viele verschiedene Klassifikationen:** FÃ¼r ein gutes Modell braucht man oft **mindestens 1000 Bilder pro Klasse**.
  - Ich habe zunÃ¤chst nur **2 Klassen** verwendet: **FussgÃ¤nger** und **Vortrittsregeln**.
  - *(Siehe Beispielbild unten)*

  <img src="/img/Blog2/Blog1_Klassifikationen.png" alt="Klassifikationen" style={{ aspectRatio: '1/1', width: '100%', height: 'auto', maxWidth: '900px' }} />


- **Viele Bounding Boxes:** Also Boxen, die zeigen, wo sich welches Feature befindet: Klasse, x1, x2, x3, x4, y1, y2, y3, y4 *(siehe YOLO Oriented Bounding Boxes)*

  <img src="/img/Blog2/aabb_vs_obb_madmann91_github.svg" alt="Bounding Boxes" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />

---

### Und jetzt?

Mit einer Programmiersprache â€“ ich benutze **Python** â€“ kann man bereits vortrainierte Modelle nutzen, um diese mit eigenen Daten weiter zu trainieren. Je komplexer das Modell, desto lÃ¤nger dauert das Training (tendenziell), aber auch die Ergebnisse werden besser.

> **FÃ¼r meine Masterarbeit verwende ich YOLOv11** â€“ ein kostenloses Modell fÃ¼r Forschungszwecke. Alternativen wÃ¤ren z.B. FastCRN oder andere.

Wie lÃ¤uft das Training ab?

#### Mein Setting
FÃ¼r diese Modelle braucht man einen **leistungsstarken PC** oder Cloud-Services. Mein Setup
- **Ryzen 7900x** und **RTX4080** (CUDA fÃ¼r YOLO Training)
- **Gute Internetverbindung** (Ã¼ber 20GB Luftbilder werden benÃ¶tigt):
- **6â€“7 Stunden Wartezeit**
<img src="/img/blog2/My_Setting.jpg" alt="MySetting" style={{ maxWidth: '50%' }} />

---

### ğŸ Python Code â€“ Schritt fÃ¼r Schritt

1. **Herunterladen aller Luftbilder**, die mit den Oriented Bounding Boxes Ã¼berschneiden.
2. **Zuschneiden** der Luftbilder in 512x512px Bilder (.tif): Die Koordinaten bleiben erhalten und sind georeferenziert.
3. **Umrechnen der Bounding Box-Koordinaten** auf relative Werte (YOLO-Standard).
4. **Modelltraining starten:**
   - Je mehr **Epochs** (Wiederholungen), desto besser lernt das Modell.
   - Je mehr **Bilder pro Durchlauf**, desto schneller das Training (aber auch hÃ¶here Hardware-Anforderungen).
   - **Komplexere Modelle** finden mehr Features (z.B. ist das Nano-Modell schlechter als das Small- oder Medium-Modell).
5. Nach ca. **5â€“6 Stunden Trainingszeit** habe ich die Bounding Boxes der erkannten Features.
6. **Umrechnung** von relativen zu absoluten Koordinaten und **Validierung der Ergebnisse**.
7. Jetzt kann das Modell **beliebig viele Luftbilder in wenigen Millisekunden bewerten**.

---

### âš™ï¸ Python: Technisch

<RevealBox title="Technische Details: YOLOv11 Training & Datenpipeline">

**Trainingsdaten:**
- Luftbilder (SWISSIMAGE, Mapillary)
- Manuell gelabelte Bounding Boxes (OBB)
- Zwei Klassen: FussgÃ¤nger, Vortrittsregeln

**Pipeline:**
1. Download & Preprocessing der Bilder
2. Zuschneiden, Georeferenzierung, Label-Konvertierung
3. Training mit YOLOv11 (Python, CUDA, RTX4080)
4. Validierung & Visualisierung der Ergebnisse

**Code-Snippet (Python):**
```python
# Beispiel: Training mit YOLOv11
from yolov11 import Trainer
trainer = Trainer(
    data='data.yaml',
    model='yolov11-obb.yaml',
    epochs=100,
    imgsz=512,
    device='cuda:0'
)
trainer.train()
```

**Hardware:** Ryzen 7900x, RTX4080, 64GB RAM

</RevealBox>
---

## ğŸ Resultat

Ich habe mein trainiertes Modell jetzt fÃ¼r die **Region ZÃ¼rich** angewendet. Hier seht ihr das Resultat als Bildergalerie. Wenn ihr zum nÃ¤chsten Post wollt, kÃ¶nnt ihr auch auf [diesen Link](/blogs/Blog3) drÃ¼cken.
<ImageGallery folder="/img/Blog2/img_galery_result" title="Ergebnisgalerie" />

---

## ğŸ–¼ï¸ Weitere Impressionen

<ImageGallery folder="/img/Blog2/img_galery" title="Originalbilder" />



---

## ğŸ”­ Ausblick

Im nÃ¤chsten Beitrag (Teil 3) geht es um die **Erarbeitung eines automatisierten Workflows**: Wie kann ich mein "Rezeptbuch" fÃ¼r die Allgemeinheit (mit Python-Erfahrung) zur VerfÃ¼gung stellen?

**Bleibt dran!**

---
Claude

