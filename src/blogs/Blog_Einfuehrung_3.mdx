export const metadata = {
  title: "Arbeitsplan Masterarbeit (Einf√ºhrung)",
  excerpt: "Mein Plan f√ºr die Organisation der Masterarbeit",
  coverImage: "/img/Blog_Einfuehrung_3/Titelbild.png",
  date: "2025-05-22T12:00:00.000Z",
  author: {
    name: "Claude Widmer",
    picture: "/img/Blog1_2/0.jpg"
  },
  tags: ["Masterarbeit"]
}

# Arbeitsplan Masterarbeit

In diesem Blogbeitrag stelle ich meinen geplanten Arbeitsablauf f√ºr die Masterarbeit vor.

Im Folgenden findest du einen visuellen Arbeitsplan mit den wichtigsten Schritten:
<div className="timeline-legend">
  <b>Legende:</b><br />
  <span></span> üü¢: Erstes Modell erstellt<br />
  <span></span> üü°: Modell in Erarbeitung<br />
  <span></span> üü†: Noch nicht begonnen
</div>


<div className="timeline-container">

  <div className="timeline-step">
    <div className="timeline-number">1</div>
    <div className="timeline-box">
      <b>Erarbeitung eines Modells (Raster) f√ºr die Modellierung des Raumes üü¢ </b><br />
      Wo kann man durchlaufen? Erstellung eines Rasters als Grundlage f√ºr die weitere Analyse.
    </div>
  </div>

  <div className="timeline-step">
    <div className="timeline-number">2</div>
    <div className="timeline-box">
      <b>Feature-Erkennung & Geolokalisierung</b>
      <ul>

        <li><b>2.0:</b> Mapillary Data Retrieval - Wie kann man √ºber eine Million Bilder vom Mapillary herunterladen und verarbeiten? üü† -> üü¢</li>

        <li><b>2.1:</b> YOLO Image Recognition ‚Äì Was ist auf den Bildern zu sehen? Verschiedene Versionen werden getestet.üü¢</li>

        <li><b>2.2:</b> Depth Estimation ‚Äì Erstellung einer Tiefenkarte.üü° -> üü¢</li>

        <li><b>2.3:</b> Zusammenf√ºhrung & Geolokalisierung der in 2.1 gefundenen Features mithilfe der Bildkoordinaten und der Tiefenkarte aus 2.2. üü† -> üü¢</li>
      </ul>
    </div>
  </div>

  <div className="timeline-step">
    <div className="timeline-number">3</div>
    <div className="timeline-box">
      <b>Klassifizierung der Features auf Schulwegsicherheitüü†</b><br />
      Anwendung von ML-Methoden oder Bewertung anhand von Literatur.
    </div>
  </div>

  <div className="timeline-step">
    <div className="timeline-number">4</div>
    <div className="timeline-box">
      <b>Webkarten-Integrationüü† -> üü° (Serverseitig fertig)</b><br />
      Integration der Ergebnisse in eine Webkarte (z.B. QGIS Web Client 2, ArcGIS, etc.).<br />
      <i>Hinweis: Einsatz von Docker zur Bereitstellung der Webanwendung.üü° -> üü¢</i>
    </div>
  </div>

</div>


<hr />

# Arbeitsplan Masterarbeit (Detailansicht)

<div className="multi-stage">

  {/* 1 ‚Äì Rastermodell */}
  <div className="main-stage">
    <div className="main-title">1. Erarbeitung eines Modells (Raster)</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Bounding Box von Z√ºrich</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">
      <div className="sub-box">
        <div className="sub-title">1.1 Raster-Erstellung</div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird erg√§nzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird erg√§nzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird erg√§nzt</li></ul>
          </>
        </div>
      </div>
    </div>

    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul><li>wird erg√§nzt</li></ul>
      </>
    </div>
  </div>

  {/* 2 ‚Äì Feature-Erkennung & Geolokalisierung */}
  <div className="main-stage">
    <div className="main-title">2. Feature-Erkennung & Geolokalisierung</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Bounding Box von Z√ºrich</li>
          <li>Mapillary API Token(s)</li>
          <li>Stammverzeichnis (z.‚ÄØB. <code>ROOT_PATH = ./</code>)</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">

      <div className="sub-box">
        <div className="sub-title">2.0 Mapillary Data Retrieval</div>
                
        <div className="sub-detail">
          <>
          <img src="/img/Blog_Einfuehrung_3/2_0_Image.png" alt="Blog Einf√ºhrung 3 - Bild 2.0" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul>
              <li><code>Bounding Box</code> von Z√ºrich</li>
              <li><code>GPKG_PATH = ./data/images_bbox_fullmeta.gpkg</code></li>
              <li>Spalte <code>thumb_1024_url</code> mit 1024p-URLs</li>
              <li><code>LAPLACIAN_THRESHOLD</code>: Threshold f√ºr die Bildunsch√§rfen-Erkennung (100)</li>
            </ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul>
              <li><strong>Laden von Geodaten:</strong> Das Notebook l√§dt Geodaten und Metadaten √ºber die Mapillary API herunter (√ºber 1,2 Millionen Bilder) und bereitet sie f√ºr die Analyse vor.</li>
              <li><strong>Erkennung von unscharfen Bildern:</strong> Es √ºberpr√ºft die Bildqualit√§t, um unscharfe Bilder zu identifizieren via Laplacian Methode (mit einem Threshold).</li>
              <li><strong>Zuordnung der Ergebnisse:</strong> Die Ergebnisse der Unsch√§rfepr√ºfung werden den Bildern zugeordnet.</li>
              <li><strong>Speichern der Daten:</strong> Die aktualisierten Geodaten werden in einer neuen GeoPackage-Datei gespeichert.</li>
            </ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul>
              <li><strong> Geopackage </strong> mit allen Metadaten von den 1.2 Millionen Bildern und die Information, ob das Bild unscharf ist.</li>
            </ul>
          </>
        </div>
      </div>

<div className="sub-box">
  <div className="sub-title">2.1 YOLO Image Recognition Training</div>
          <div className="sub-detail">
          <>
          <img src="/img/Blog_Einfuehrung_3/2_1_Image.png" alt="Blog Einf√ºhrung 3 - Bild 2.1" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
          </>
        </div>
  <div className="sub-detail">
    <>
      <strong>Input:</strong>
      <ul>
        <li>Mapillary Segmentation and Object Detection Dataset</li>
        <li>YOLO-kompatible Labeldateien (YOLOv8 Format)</li>
      </ul>
    </>
  </div>
  <div className="sub-detail">
    <>
      <strong>Algorithmus:</strong>
      <ul>
        <li>Laden von Trainings- und Validierungsdaten aus YOLO-kompatiblen Pfaden</li>
        <li>Verwendung eines YOLOv8-Medium-Modells (<code>yolo11m-seg.pt</code>) mit vortrainierten Gewichten</li>
        <li>Durchf√ºhrung eines Segmentierungs- und Objekterkennungs-Trainings mit Mixed Precision (AMP)</li>
        <li>Anwendung von Transfer Learning und Feinjustierung √ºber 100+ Epochen mit Early Stopping</li>
        <li>Optimizer automatisch gew√§hlt (AdamW) mit automatischer Lernratenanpassung</li>
        <li>Evaluation nach jeder Epoche mit mAP, Precision und Recall</li>
        <li>Training und Vergleich unterschiedlicher Modellgr√∂ssen (n, s, m, l)</li>
        <li>√úber 12‚ÄØStunden Trainingszeit f√ºr das Medium-Modell, insgesamt √ºber 48‚ÄØStunden zur Modellevaluierung (RTX 4080, 64‚ÄØGB RAM)</li>
      </ul>
    </>
  </div>
  <div className="sub-detail">
    <>
      <strong>Output:</strong>
      <ul>
        <li>Trainiertes YOLO-Modell (<code>best.pt</code>)</li>
        <li>Evaluationsmetriken (Precision, Recall, mAP50, mAP50-95)</li>
        <li>Trainingsplots und Ergebnisvisualisierungen</li>
      </ul>
    </>
  </div>
<div className="sub-detail">
      <>
        <strong>Probleme:</strong>
        <ul>
          <li>Aufgrund des sehr grossen Datensatzes (√ºber 18'000 Bilder) konnte nicht der gesamte Umfang f√ºr das Training genutzt werden.</li>
          <li>Die verf√ºgbare Rechenleistung (GPU/VRAM) war limitiert, was zu kleineren Batchgr√∂ssen und k√ºrzeren Trainingszeiten f√ºhrte.</li>
          <li>Dadurch litt insbesondere der <strong>Recall</strong> (Erkennungsrate), da das Modell viele Objekte nicht zuverl√§ssig detektierte.</li>
          <li>Zus√§tzlich erschwerten die grosse Klassenvielfalt und unbalancierte Verteilung die Generalisierung.</li>
        </ul>
      </>
    </div>
</div>

  <div className="sub-box">
    <div className="sub-title">2.2 Depth Estimation Processing</div>
    <div className="sub-detail">
      <>
      <img src="/img/Blog_Einfuehrung_3/2_2_Image.png" alt="Blog Einf√ºhrung 3 - Bild 2.2" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Die 1.2 Millionen Bilder vom Schritt 2.0</li>
          <li>Pfadliste aller validen Bilder (vorvalidiert via PIL)</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Algorithmus:</strong>
        <ul>
          <li>Scannen aller .jpg-Dateien im Datensatzordner</li>
          <li>Parallelvalidierung mittels PIL, um defekte Bilder zu filtern</li>
          <li>Reduktion der Bildaufl√∂sung (50‚ÄØ%) zur Performance-Optimierung</li>
          <li>Batchweise Tiefensch√§tzung mittels <code>depth-anything</code> Pipeline (HuggingFace)</li>
          <li>Optionales Invertieren der Tiefenwerte f√ºr GIS-Konsistenz</li>
          <li>Export der Tiefendaten als .npz (komprimiert, float16)</li>
          <li>Optional: Export von Tiefenbildern und Histogrammen zur visuellen Kontrolle</li>
          <li>Speicherbereinigung und GPU-Freigabe nach jeder Batch f√ºr stabile Laufzeit</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Tiefenkarten im .npz-Format (komprimiert, float16)</li>
          <li>Optional: Visualisierungen der Tiefenbilder (.png)</li>
          <li>Optional: Histogramme der Tiefenverteilung pro Bild</li>
          <li>Gespeichert im Unterordner <code>depth_processed/</code></li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Probleme:</strong>
        <ul>
          <li>Volle Aufl√∂sung f√ºhrte zu Out-of-Memory-Fehlern ‚Üí Downscaling n√∂tig</li>
          <li>Einige Bilder konnten trotz vorheriger Validierung nicht verarbeitet werden (Pipeline-Fehler)</li>
          <li>Speicherfreigabe und Garbage Collection war n√∂tig, um GPU-Nutzung stabil zu halten</li>
          <li>Laufzeit von √ºber 12‚ÄØStunden f√ºr alle Bilder (RTX 4080, 64‚ÄØGB RAM, 128er Batch-Gr√∂sse)</li>
          <li>Hohe Modellqualit√§t, aber keine Echtzeitverarbeitung m√∂glich</li>
        </ul>
      </>
    </div>
  </div>


  <div className="sub-box">
    <div className="sub-title">2.3 YOLO Object Detection Processing</div>
    <div className="sub-detail">
      <>
      <img src="/img/Blog_Einfuehrung_3/2_3_Image.jpg" alt="Blog Einf√ºhrung 3 - Bild 2.3" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Gefilterte Bildliste aus Schritt 2.2 (inkl. validem Tiefendatenpfad)</li>
          <li>Vortrainiertes YOLO-Modell (<code>.pt</code>)</li>
          <li>Konfigurationsdatei mit Schwellenwerten, Pfaden und Parametern</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Algorithmus:</strong>
        <ul>
          <li>Filterung aller Bilder ohne Tiefenkarte oder bei Unschaerfe</li>
          <li>Batchweises Kopieren der Bilder in tempor√§ren Inferenzordner</li>
          <li>Ausf√ºhrung der YOLO-Inferenz (<code>ultralytics.YOLO</code>) in Batches</li>
          <li>Speichern der Ergebnisse als <code>.npz</code> pro Bild (BBox, Klassen, Konfidenzen, Maske)</li>
          <li>Zusammenf√ºhrung von Tiefen- und YOLO-Daten pro Objekt (inkl. Tiefenwertbestimmung)</li>
          <li>Kategorisierung der Objekte anhand der Tiefe (z.B. "near", "medium", "far")</li>
          <li>Optional: Visualisierung der Objekte mit Tiefenklassen</li>
          <li>Speichern aller Objekte in <code>.parquet</code>-Datei (schnell + komprimiert)</li>
          <li>Geometrie-Zuordnung via <code>GeoPackage</code> & Polars f√ºr performantes Join</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Output:</strong>
        <ul>
          <li>YOLO-Ergebnisse als komprimierte <code>.npz</code>-Dateien</li>
          <li>Vereinte Objektliste mit Tiefe, Klasse, Konfidenz und Koordinaten (<code>.parquet</code>)</li>
          <li>Optional: Visualisierte Objekte mit Bounding Boxes und Tiefenklassen (.jpg)</li>
          <li>Finale Objekt-Geodaten mit X/Y-Koordinaten aus GPKG</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Probleme:</strong>
        <ul>
          <li>Inferenzzeit √ºber 30‚ÄØStunden bei 1.2‚ÄØMio Bildern ‚Üí kein Echtzeitbetrieb</li>
          <li>Fehlerhafte oder fehlende <code>.npz</code>-Dateien mussten manuell gefiltert werden</li>
          <li>Initial hohe RAM/GPU-Auslastung ‚Äì Speicherbereinigung und batchweises Laden notwendig</li>
          <li>Konvertierung von GPKG zu Parquet n√∂tig f√ºr effizientes Polars-Join</li>
          <li>Batch-Gr√∂sse musste angepasst werden, um stabile Performance auf RTX 4080 zu erreichen</li>
        </ul>
      </>
    </div>
  </div>


  </div>
    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Geopackage-Datei mit Metadaten zu √ºber 1.2 Millionen Bildern (inkl. Unsch√§rfe-Information)</li>
          <li>Trainiertes YOLO-Modell zur Objekterkennung (<code>best.pt</code>)</li>
          <li>Tiefenkarten zu validen Bildern im <code>.npz</code>-Format (komprimiert, float16)</li>
          <li>Gefilterte Liste valider Bilder mit vorhandener Tiefenkarte und guter Bildqualit√§t</li>
          <li>YOLO-Erkennungsergebnisse pro Bild als <code>.npz</code>-Dateien (BBox, Klassen, Konfidenzen)</li>
          <li>Kombinierte Objektdaten mit Tiefe und Kategorisierung ("near", "medium", "far") als <code>.parquet</code></li>
          <li>Finale Objektliste mit zugeordneten Koordinaten (via Geometrie-Join) f√ºr Mapping-Tools</li>
          <li>Optional: Visualisierte Objekte mit Tiefenklassifikation (.jpg) f√ºr Qualit√§tspr√ºfung</li>
        </ul>
      </>
    </div>

    <div className="main-output">
      <>
        <strong>Probleme & Aufwand:</strong>
        <ul>
          <li>Extrem hoher Rechenaufwand: Die gesamte Verarbeitung von Bild-Download, Tiefensch√§tzung und YOLO-Inferenz dauerte mehrere **Tage**, trotz RTX 4080 und 64‚ÄØGB RAM.</li>
          <li>Der Schritt 2.2 (Tiefensch√§tzung) ben√∂tigte √ºber **12 Stunden** allein f√ºr die validen Bilder.</li>
          <li>Die Inferenz mit YOLO in Schritt 2.3 dauerte √ºber **30 Stunden**, was Echtzeitverarbeitung ausschliesst.</li>
          <li>Hohes Risiko von Out-of-Memory-Fehlern, insbesondere bei voller Aufl√∂sung ‚Üí Reduktion der Bildgr√∂sse war zwingend.</li>
          <li>Manuelle Filterung notwendig bei fehlerhaften oder leeren <code>.npz</code>-Ergebnissen</li>
          <li>Speicherfreigabe und Garbage Collection mussten explizit eingebaut werden, um GPU-Laufzeit zu stabilisieren.</li>
          <li>Join von Geometrien auf √ºber 1 Million Objekte erforderte Konvertierung von GPKG zu Parquet und Einsatz von Polars f√ºr Performance.</li>
        </ul>
        <p><i>Fazit:</i> Die Datenverarbeitung in Phase 2 war technisch erfolgreich, aber extrem zeit- und ressourcenintensiv. F√ºr produktive Anwendungen m√ºsste massiv skaliert oder auf vorverarbeitete Daten zur√ºckgegriffen werden.</p>
      </>
    </div>
  </div>

</div>
