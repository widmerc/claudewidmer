export const metadata = {
  title: "Arbeitsplan Masterarbeit (Stand August)",
  excerpt: "Mein Plan für die Organisation der Masterarbeit",
  coverImage: "/img/Blog_Einfuehrung_3/Titelbild.jpg",
  date: "2025-05-21T12:00:00.000Z",
  author: {
    name: "Claude Widmer",
    picture: "/img/Blog1_2/0.jpg"
  },
  tags: ["Masterarbeit"],
  ogImage: {
    url: '/img/Blog_Einfuehrung_3/Titelbild.jpg',
  },
}

# Arbeitsplan Masterarbeit

In diesem Blogbeitrag stelle ich meinen geplanten Arbeitsablauf für die Masterarbeit vor.

Im Folgenden findest du einen visuellen Arbeitsplan mit den wichtigsten Schritten:

import Image from 'next/image'
import RevealBox from "@/app/_components/RevealBox";

<div className="multi-stage">

  {/* 1 – Vektormodell */}
  <div className="main-stage">
    <div className="main-title">1. Erarbeitung eines Modells (Vektor)</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Bounding Box von Zürich</li>
          <li>Netzwerk von Zürich</li>
          <li>OSM Zugriff</li>
        </ul>
      </>
    </div>

    
    <div className="main-output">
  <>
    <strong>Output:</strong>
    <ul>
      <li>Vollstaendig bereinigtes und harmonisiertes Fusswegnetz von Zürich</li>
      <li>Knoten- und Kantenstruktur (Graph-Modell) mit konsistenten IDs</li>
      <li>Dokumentierte Attribute pro Segment (Laenge, Zugang, Quelle, Klassifikation)</li>
    </ul>
    <strong>Herausforderungen:</strong>
    <ul>
      <li>Unterschiedliche Datenqualitaet und Abdeckung zwischen OSM und OGD-Zürich</li>
      <li>Topologische Konsistenz: Lücken, Ueberschneidungen und Duplikate sauber bereinigen</li>
      <li>Automatisierte Abgrenzung von irrelevanten Segmenten (z. B. kurze Artefakte, private Wege)</li>
      <li>Skalierbarkeit bei grossen Datenmengen und effiziente Geoprocessing-Workflows</li>
    </ul>
  </>
</div>

    <div className="sub-steps">
      <div className="sub-box">
        <div className="sub-title">1.1 OSM-Netzwerkaufbereitung</div>
            <div className="sub-detail">
      <>
      <Image src="/img/Blog_Einfuehrung_3/1_1_Image.jpg" alt="Blog Einführung 3 - Bild 2.3" style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </>
    </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul>              <li><code>Bounding Box</code> von Zürich (LV95)</li>
              <li>Geofabrik/OSM Fussweg-Daten (Ways & Nodes)</li>
              <li><code>EPSG:2056</code> Ziel-Koordinatensystem</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul>              <li>Download des aktuellen OSM-Extrakts und Filtern auf fussrelevante <code>highway</code>-Typen (footway, path, steps, living_street, service mit Fusszugang)</li>
              <li>Transformation in <code>EPSG:2056</code> und Zuschnitt auf die Bounding Box</li>
              <li>Aufteilen multiliner Geometrien; Normalisierung von Richtungen</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul>              <li>Bereinigtes, auf Zürich zugeschnittenes Liniennetz (GPKG)</li>
              <li>Konsistente Attribute: <code>highway</code>, <code>surface</code>, <code>foot</code>, <code>name</code></li></ul>
          </>
        </div>
      </div>

      <div className="sub-box">
        <div className="sub-title">1.2 Integration Stadt Zürich & OSM</div>
            <div className="sub-detail">
      <>
      <Image src="/img/Blog_Einfuehrung_3/1_2_Image.jpg" alt="Blog Einführung 3 - Bild 2.3" style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </>
    </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul>              <li>OGD-Stadt Zürich Fussweg-/Trottoir-Layer</li>
              <li>OSM-Liniennetz aus 1.1 </li>
              <li><code>Snap-Toleranz</code> (z. B. 0.5–1.0 m)</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul>              <li>Beide Datensätze nach <code>EPSG:2056</code> transformieren</li>
              <li>Vereinheitlichung der Attribute (Feldmapping) und Verschneiden/Union der Layer</li>
              <li>Schliessen kleiner Lücken & Höhlen (&lt; 10 m²) und Liniensnap an Knoten im Toleranzradius</li>
              <li>Aufloesen von Duplikaten, <code>dissolve</code> nach relevanten Attributen</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul>              <li>Zusammengeführtes, topologisch sauberes Fusswegnetz</li>
              <li>Dokumentierte Feldliste (Datenkatalog) und Herkunftskennzeichen pro Segment</li></ul>
          </>
        </div>
      </div>

      <div className="sub-box">
        <div className="sub-title">1.3 Graph-Erstellung & Qualitätsprüfung</div>
            <div className="sub-detail">
      <>
      <Image
        src="/img/Blog_Einfuehrung_3/1_3_Image.jpg"
        alt="Blog Einführung 3 - Bild 2.3"
        width={400}
        height={250}
        style={{ width: '100%', height: 'auto', maxWidth: '800px' }}
      />
      </>
    </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul>              <li>Bereinigtes Liniennetz aus 1.2</li>
              <li>Schwellwerte für Längen-/Qualitaetsfilter</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul>              <li>Erzeugung eines ungerichteten Graphen (Knoten=Endpunkte, Kanten=Liniensegmente)</li>
              <li>Berechnung von Kantenlaenge</li>
              <li>Entfernung sehr kurzer Artefaktkanten (Projekt-spezifischer Schwellwert von 20m)</li>
              <li>Prüfung zusammenhaengender Komponenten; Sicherstellen globaler Erreichbarkeit</li>
              <li>Export der Graph-Struktur (Knoten/Kanten) inkl. eindeutiger IDs</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul>              <li>Routbares Fusswegnetz mit Attributen (Länge)</li>
              <li>Knoten-/Kantentabellen (GPKG) als Basis für 2.x Erkennung & 3.x Analysen</li></ul>
          </>
        </div>
      </div>
    </div>

    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Vollstaendig bereinigtes und harmonisiertes Fusswegnetz von Zürich</li>
          <li>Knoten- und Kantenstruktur (Graph-Modell) mit konsistenten IDs</li>
          <li>Dokumentierte Attribute pro Segment (Laenge, Zugang, Quelle, Klassifikation)</li>
        </ul>
        <strong>Herausforderungen:</strong>
        <ul>
          <li>Unterschiedliche Datenqualitaet und Abdeckung zwischen OSM und OGD-Zürich</li>
          <li>Topologische Konsistenz: Lücken, Ueberschneidungen und Duplikate sauber bereinigen</li>
          <li>Automatisierte Abgrenzung von irrelevanten Segmenten (z. B. kurze Artefakte, private Wege)</li>
          <li>Skalierbarkeit bei grossen Datenmengen und effiziente Geoprocessing-Workflows</li>
        </ul>
      </>
    </div>

  </div>

  {/* 2 – Feature-Erkennung & Geolokalisierung */}
  <div className="main-stage">
    <div className="main-title">2. Feature-Erkennung & Geolokalisierung</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Bounding Box von Zürich</li>
          <li>Mapillary API Token(s)</li>
          <li>Stammverzeichnis (z. B. <code>ROOT_PATH = ./</code>)</li>
        </ul>
      </>
    </div>

    
    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Vollständig bereinigtes und harmonisiertes Fusswegnetz von Zürich</li>
          <li>Knoten- und Kantenstruktur (Graph-Modell) mit konsistenten IDs</li>
          <li>Dokumentierte Attribute pro Segment (Länge, Zugang, Quelle, Klassifikation)</li>
        </ul>
        <strong>Herausforderungen:</strong>
        <ul>
          <li>Unterschiedliche Datenqualität und Abdeckung zwischen OSM und OGD-Zürich</li>
          <li>Topologische Konsistenz: Lücken, Überschneidungen und Duplikate sauber bereinigen</li>
          <li>Automatisierte Abgrenzung von irrelevanten Segmenten (z. B. kurze Artefakte, private Wege)</li>
          <li>Skalierbarkeit bei grossen Datenmengen und effiziente Geoprocessing-Workflows</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">

      <div className="sub-box">
        <div className="sub-title">2.0 Mapillary Data Retrieval</div>
                
        <div className="sub-detail">
          <>
          <Image src="/img/Blog_Einfuehrung_3/2_0_Image.png" alt="Blog Einführung 3 - Bild 2.0" style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul>
              <li><code>Bounding Box</code> von Zürich</li>
              <li><code>GPKG_PATH = ./data/images_bbox_fullmeta.gpkg</code></li>
              <li>Spalte <code>thumb_1024_url</code> mit 1024p-URLs</li>
              <li><code>LAPLACIAN_THRESHOLD</code>: Threshold für die Bildunschärfen-Erkennung (100)</li>
            </ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul>
              <li><strong>Laden von Geodaten:</strong> Das Notebook lädt Geodaten und Metadaten über die Mapillary API herunter (über 1,2 Millionen Bilder) und bereitet sie für die Analyse vor.</li>
              <li><strong>Erkennung von unscharfen Bildern:</strong> Es überprüft die Bildqualität, um unscharfe Bilder zu identifizieren via Laplacian Methode (mit einem Threshold).</li>
              <li><strong>Zuordnung der Ergebnisse:</strong> Die Ergebnisse der Unschärfeprüfung werden den Bildern zugeordnet.</li>
              <li><strong>Speichern der Daten:</strong> Die aktualisierten Geodaten werden in einer neuen GeoPackage-Datei gespeichert.</li>
            </ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul>
              <li><strong> Geopackage </strong> mit allen Metadaten von den 1.2 Millionen Bildern und die Information, ob das Bild unscharf ist.</li>
            </ul>
          </>
        </div>
      </div>

<div className="sub-box">
  <div className="sub-title">2.1 YOLO Image Recognition Training</div>
          <div className="sub-detail">
          <>
          <Image src="/img/Blog_Einfuehrung_3/2_1_Image.png" alt="Blog Einführung 3 - Bild 2.1" style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
          </>
        </div>
  <div className="sub-detail">
    <>
      <strong>Input:</strong>
      <ul>
        <li>Mapillary Segmentation and Object Detection Dataset</li>
        <li>YOLO-kompatible Labeldateien (YOLOv8 Format)</li>
      </ul>
    </>
  </div>
  <div className="sub-detail">
    <>
      <strong>Algorithmus:</strong>
      <ul>
        <li>Laden von Trainings- und Validierungsdaten aus YOLO-kompatiblen Pfaden</li>
        <li>Verwendung eines YOLOv8-Medium-Modells (<code>yolo11m-seg.pt</code>) mit vortrainierten Gewichten</li>
        <li>Durchführung eines Segmentierungs- und Objekterkennungs-Trainings mit Mixed Precision (AMP)</li>
        <li>Anwendung von Transfer Learning und Feinjustierung über 100+ Epochen mit Early Stopping</li>
        <li>Optimizer automatisch gewählt (AdamW) mit automatischer Lernratenanpassung</li>
        <li>Evaluation nach jeder Epoche mit mAP, Precision und Recall</li>
        <li>Training und Vergleich unterschiedlicher Modellgrössen (n, s, m, l)</li>
        <li>Über 12 Stunden Trainingszeit für das Medium-Modell, insgesamt über 48 Stunden zur Modellevaluierung (RTX 4080, 64 GB RAM)</li>
      </ul>
    </>
  </div>
  <div className="sub-detail">
    <>
      <strong>Output:</strong>
      <ul>
        <li>Trainiertes YOLO-Modell (<code>best.pt</code>)</li>
        <li>Evaluationsmetriken (Precision, Recall, mAP50, mAP50-95)</li>
        <li>Trainingsplots und Ergebnisvisualisierungen</li>
      </ul>
    </>
  </div>
<div className="sub-detail">
      <>
        <strong>Herausforderungen:</strong>
        <ul>
          <li>Aufgrund des sehr grossen Datensatzes (über 18'000 Bilder) konnte nicht der gesamte Umfang für das Training genutzt werden.</li>
          <li>Die verfügbare Rechenleistung (GPU/VRAM) war limitiert, was zu kleineren Batchgrössen und kürzeren Trainingszeiten führte.</li>
          <li>Dadurch litt insbesondere der <strong>Recall</strong> (Erkennungsrate), da das Modell viele Objekte nicht zuverlässig detektierte.</li>
          <li>Zusätzlich erschwerten die grosse Klassenvielfalt und unbalancierte Verteilung die Generalisierung.</li>
        </ul>
      </>
    </div>
</div>

  <div className="sub-box">
    <div className="sub-title">2.2 Depth Estimation Processing</div>
    <div className="sub-detail">
      <>
      <Image src="/img/Blog_Einfuehrung_3/2_2_Image.png" alt="Blog Einführung 3 - Bild 2.2" style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Die 1.2 Millionen Bilder vom Schritt 2.0</li>
          <li>Pfadliste aller validen Bilder (vorvalidiert via PIL)</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Algorithmus:</strong>
        <ul>
          <li>Scannen aller .jpg-Dateien im Datensatzordner</li>
          <li>Parallelvalidierung mittels PIL, um defekte Bilder zu filtern</li>
          <li>Reduktion der Bildauflösung (50 %) zur Performance-Optimierung</li>
          <li>Batchweise Tiefenschätzung mittels <code>depth-anything</code> Pipeline (HuggingFace)</li>
          <li>Optionales Invertieren der Tiefenwerte für GIS-Konsistenz</li>
          <li>Export der Tiefendaten als .npz (komprimiert, float16)</li>
          <li>Optional: Export von Tiefenbildern und Histogrammen zur visuellen Kontrolle</li>
          <li>Speicherbereinigung und GPU-Freigabe nach jeder Batch für stabile Laufzeit</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Tiefenkarten im .npz-Format (komprimiert, float16)</li>
          <li>Optional: Visualisierungen der Tiefenbilder (.png)</li>
          <li>Optional: Histogramme der Tiefenverteilung pro Bild</li>
          <li>Gespeichert im Unterordner <code>depth_processed/</code></li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Herausforderungen:</strong>
        <ul>
          <li>Volle Auflösung führte zu Out-of-Memory-Fehlern → Downscaling nötig</li>
          <li>Einige Bilder konnten trotz vorheriger Validierung nicht verarbeitet werden (Pipeline-Fehler)</li>
          <li>Speicherfreigabe und Garbage Collection war nötig, um GPU-Nutzung stabil zu halten</li>
          <li>Laufzeit von über 12 Stunden für alle Bilder (RTX 4080, 64 GB RAM, 128er Batch-Grösse)</li>
          <li>Hohe Modellqualität, aber keine Echtzeitverarbeitung möglich</li>
        </ul>
      </>
    </div>
  </div>


  <div className="sub-box">
    <div className="sub-title">2.3 YOLO Object Detection Processing</div>
    <div className="sub-detail">
      <>
      <Image src="/img/Blog_Einfuehrung_3/2_3_Image.jpg" alt="Blog Einführung 3 - Bild 2.3" style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Gefilterte Bildliste aus Schritt 2.2 (inkl. validem Tiefendatenpfad)</li>
          <li>Vortrainiertes YOLO-Modell (<code>.pt</code>)</li>
          <li>Konfigurationsdatei mit Schwellenwerten, Pfaden und Parametern</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Algorithmus:</strong>
        <ul>
          <li>Filterung aller Bilder ohne Tiefenkarte oder bei Unschaerfe</li>
          <li>Batchweises Kopieren der Bilder in temporären Inferenzordner</li>
          <li>Ausführung der YOLO-Inferenz (<code>ultralytics.YOLO</code>) in Batches</li>
          <li>Speichern der Ergebnisse als <code>.npz</code> pro Bild (BBox, Klassen, Konfidenzen, Maske)</li>
          <li>Zusammenführung von Tiefen- und YOLO-Daten pro Objekt (inkl. Tiefenwertbestimmung)</li>
          <li>Kategorisierung der Objekte anhand der Tiefe (z.B. "near", "medium", "far")</li>
          <li>Optional: Visualisierung der Objekte mit Tiefenklassen</li>
          <li>Speichern aller Objekte in <code>.parquet</code>-Datei (schnell + komprimiert)</li>
          <li>Geometrie-Zuordnung via <code>GeoPackage</code> & Polars für performantes Join</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Output:</strong>
        <ul>
          <li>YOLO-Ergebnisse als komprimierte <code>.npz</code>-Dateien</li>
          <li>Vereinte Objektliste mit Tiefe, Klasse, Konfidenz und Koordinaten (<code>.parquet</code>)</li>
          <li>Optional: Visualisierte Objekte mit Bounding Boxes und Tiefenklassen (.jpg)</li>
          <li>Finale Objekt-Geodaten mit X/Y-Koordinaten aus GPKG</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Herausforderungen:</strong>
        <ul>
          <li>Inferenzzeit über 30 Stunden bei 1.2 Mio Bildern → kein Echtzeitbetrieb</li>
          <li>Fehlerhafte oder fehlende <code>.npz</code>-Dateien mussten manuell gefiltert werden</li>
          <li>Initial hohe RAM/GPU-Auslastung – Speicherbereinigung und batchweises Laden notwendig</li>
          <li>Konvertierung von GPKG zu Parquet nötig für effizientes Polars-Join</li>
          <li>Batch-Grösse musste angepasst werden, um stabile Performance auf RTX 4080 zu erreichen</li>
        </ul>
      </>
    </div>
  </div>

<div className="sub-box">
  <div className="sub-title">2.4 Object Geolocation</div>

  <div className="sub-detail">
    <Image src="/img/Blog_Einfuehrung_3/2_4_Image.jpg" 
      alt="Objekt-Geolokalisierung: von Bild-Detektion zu LV95-Koordinaten" 
      style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
  </div>

  <div className="sub-detail">
    <strong>Input:</strong>
    <ul>
      <li>YOLO-Detektionen als <code>.parquet</code> (BBox, Klasse, Konfidenz, z_class)</li>
      <li>Mapillary GPKG mit Kameraposition und Metadaten (z. B. <code>id</code>, <code>is_pano</code>, <code>computed_compass_angle</code>, <code>computed_geometry</code>)</li>
      <li>Parameter: Bildbreite (<code>image_width</code>) und Sichtfeld (<code>fov</code>)</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Algorithmus (ohne OSM):</strong>
    <ul>
      <li>
        Join YOLO ↔ Mapillary:
        <ul>
          <li>Match über <code>image_id</code> (YOLO) und <code>id</code> (Mapillary); Filtere irrelevante Labels und sehr ferne z-Klassen.</li>
          <li>Erzeuge Geometrie aus <code>computed_geometry</code> (WGS84) und projiziere nach LV95 (<code>EPSG:2056</code>).</li>
        </ul>
      </li>
      <li>
        Winkel- und Distanz-Adjustierung pro Objekt:
        <ul>
          <li>Bestimme BBox-Zentrum (<code>cx</code>) und berechne Blickrichtungs-Offset: <code>((cx / image_width) − 0.5) * fov</code>.</li>
          <li>Finaler Azimut = <code>computed_compass_angle + Offset</code>.</li>
          <li>Verschiebe den Kamera-Punkt entlang dieses Winkels um eine z-klassenabhängige Distanz (z. B. <b>very near: 5 m</b>, <b>near: 7.5 m</b>, <b>medium: 20 m</b>).</li>
        </ul>
      </li>
      <li>Parallelisierung in Chunks, Schreiben als <code>.parquet</code> (LV95-Punkte mit <code>x_adj</code>/<code>y_adj</code>).</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Output:</strong>
    <ul>
      <li><code>joined_dataset.parquet</code> – verknüpfte Rohdaten (Kamerapositionen)</li>
      <li><code>adjusted_dataset.parquet</code> – geolokalisierte Objektpunkte (LV95) mit Adjustierung</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Herausforderungen:</strong>
    <ul>
      <li>Approximative Distanzen aus z-Klassen → Positionsfehler abhängig von FOV und BBox-Lage</li>
      <li>Unterschiedliche Bildtypen (Panorama vs. Nicht-Panorama)</li>
      <li>Performance bei sehr grossen Datensätzen → Multiprocessing, Parquet einsetzen</li>
    </ul>
  </div>
</div>





<div className="sub-box">
  <div className="sub-title">2.5 YOLO Objekterkennung auf SWISSIMAGE Orthofotos</div>
  <div className="sub-detail">
    <Image src="/img/Blog_Einfuehrung_3/2_5_Image.jpg" 
      alt="Blog Einführung 3 - Bild 2.5" 
      style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
  </div>

  <div className="sub-detail">
    <strong>Input:</strong>
    <ul>
      <li>SWISSIMAGE Orthofotos von Zürich (170 km² ≈ 120'000 px × 140'000 px)</li>
      <li>8'000 manuell digitalisierte Trainingsfeatures für das YOLO-Modell</li>
      <li>OSM-Daten für Tramlinien und Tramnetz im Kanton Zürich</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Algorithmus:</strong>
    <ul>
      <li>
        Entwicklung von vier YOLO-Modellen:
        <ul>
          <li>Modell 1: YOLOv11L – 100 Epochs – Klassen: 30er-Zone, Auto, Fussgängerstreifen, Pfeilmarkierung, Schule, Tram, Vortritt und Zug</li>
          <li>Modell 2: YOLOv11X – 70 Epochs – Fokus Tram im Kanton Zürich</li>
          <li>Modell 3: YOLOv11X – 100 Epochs – Fokus Strassenmarkierungen in Luzern</li>
          <li>Modell 4: YOLOv11L – 100 Epochs – Fokus Tram in der Stadt Bern (mit OSM-Daten)</li>
        </ul>
      </li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Output:</strong>
    <ul>
      <li>YOLO-Ergebnisse als <code>.gpkg</code>-Dateien</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Herausforderungen:</strong>
    <ul>
      <li>YOLO-Training dauerte über 6 Stunden</li>
      <li>Interferenzzeit aller vier Modelle inkl. Zusammenführung: mehr als 11.5 Stunden</li>
      <li>Hohe RAM/GPU-Auslastung – viel Ausprobieren nötig für optimale Batchgrösse, Bildgrösse und Modellwahl</li>
      <li>Allgemeine Big-Data-Probleme (alles wurde lokal auf dem PC berechnet)</li>
    </ul>
  </div>
</div>



  </div>
    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Geopackage-Datei mit Metadaten zu über 1.2 Millionen Bildern (inkl. Unschärfe-Information)</li>
          <li>Trainiertes YOLO-Modell zur Objekterkennung (<code>best.pt</code>)</li>
          <li>Tiefenkarten zu validen Bildern im <code>.npz</code>-Format (komprimiert, float16)</li>
          <li>Gefilterte Liste valider Bilder mit vorhandener Tiefenkarte und guter Bildqualität</li>
          <li>YOLO-Erkennungsergebnisse pro Bild als <code>.npz</code>-Dateien (BBox, Klassen, Konfidenzen)</li>
          <li>Kombinierte Objektdaten mit Tiefe und Kategorisierung ("near", "medium", "far") als <code>.parquet</code></li>
          <li>Finale Objektliste mit zugeordneten Koordinaten (via Geometrie-Join) für Mapping-Tools</li>
          <li>Optional: Visualisierte Objekte mit Tiefenklassifikation (.jpg) für Qualitätsprüfung</li>
        </ul>
      </>
    </div>

    <div className="main-output">
      <>
        <strong>Probleme & Aufwand:</strong>
        <ul>
          <li>Extrem hoher Rechenaufwand: Die gesamte Verarbeitung von Bild-Download, Tiefenschätzung und YOLO-Inferenz dauerte mehrere **Tage**, trotz RTX 4080 und 64 GB RAM.</li>
          <li>Der Schritt 2.2 (Tiefenschätzung) benötigte über **12 Stunden** allein für die validen Bilder.</li>
          <li>Die Inferenz mit YOLO in Schritt 2.3 dauerte über **30 Stunden**, was Echtzeitverarbeitung ausschliesst.</li>
          <li>Hohes Risiko von Out-of-Memory-Fehlern, insbesondere bei voller Auflösung → Reduktion der Bildgrösse war zwingend.</li>
          <li>Manuelle Filterung notwendig bei fehlerhaften oder leeren <code>.npz</code>-Ergebnissen</li>
          <li>Speicherfreigabe und Garbage Collection mussten explizit eingebaut werden, um GPU-Laufzeit zu stabilisieren.</li>
          <li>Join von Geometrien auf über 1 Million Objekte erforderte Konvertierung von GPKG zu Parquet und Einsatz von Polars für Performance.</li>
        </ul>
        <p><i>Fazit:</i> Die Datenverarbeitung in Phase 2 war technisch erfolgreich, aber extrem zeit- und ressourcenintensiv. Ich bin sehr froh, hat es geklappt.</p>
      </>
    </div>
  </div>


<div className="main-stage">
  <div className="main-title">3. Klassifikation der Schulwegsicherheit</div>

  <div className="main-input">
    <div>
      Ziel dieses Schritts ist es, aus den vorbereiteten Daten (YOLO-Detektionen auf Orthofotos und Street-Level-Fotos,
      Ground-Truth aus Zürich sowie Netzwerkdaten) Sicherheitsbewertungen für einzelne Strassen- und Wegsegmente zu
      berechnen. Diese Bewertungen werden auf zwei Arten erstellt:
    </div>
    <ul>
      <li><b>Machine Learning</b> – das Modell lernt aus den echten Bewertungen der Stadtpolizei Zürich.</li>
      <li><b>Regelbasiert</b> – ein einfaches Punktesystem mit Abzügen und Boni.</li>
    </ul>
    <div>
      Am Ende lassen sich mit diesen Bewertungen sichere Routen berechnen. Dafür wird eine Kostenfunktion genutzt,
      die Länge und Sicherheit kombiniert. Zusätzlich wurde ein einfaches API erstellt, das auf Basis von Start- und
      Zielpunkten die Route mit den genutzten Kanten zurückgibt.
    </div>
  </div>

  <div className="sub-steps">
    <div className="sub-box">
      <div className="sub-title">3.1 Datenabgleich SWISSIMAGE (YOLO-Polygone)</div>

      <div className="sub-detail">
        <Image src="/img/Blog_Einfuehrung_3/3_1_Image.jpg"
          alt="SWISSIMAGE: von YOLO-Polygonen zu Anteilen je Strassen-Segment"
          style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </div>

      <div className="sub-detail">
        <strong>Input:</strong>
        <ul>
          <li>Strassennetz (Kanten/Segmente)</li>
          <li>YOLO-Detektionen auf SWISSIMAGE-Orthofotos als <b>Polygone</b> (z. B. 30er-Zonen, Velowege, Tramgleise, Markierungen)</li>
          <li>Parameter: Buffer je Klasse</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Algorithmus:</strong>
        <ul>
          <li>Polygone bereinigen und je Klasse pufferen</li>
          <li>Overlay mit Kanten → Coverage je Klasse (Längenanteil)</li>
          <li>Pivot zu Feature-Spalten; fehlende Werte = 0</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Output:</strong>
        <ul>
          <li>`edges_poly.parquet` – Kanten mit Klassenanteilen (0–1) und `länge_m`</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Bildvorschlag:</strong>
        <ul>
          <li>Orthofoto-Hintergrund, farbige YOLO-Polygone (z. B. 30er-Zone grün, Tramgleis violett)</li>
          <li>Strassensegmente mit Popups „Anteil 30er-Zone = 0.6“</li>
          <li>Kleine Legende der Klassenfarben</li>
        </ul>
      </div>
    </div>

    <div className="sub-box">
      <div className="sub-title">3.2 Datenabgleich Street-Level (YOLO-Punkte)</div>

      <div className="sub-detail">
        <Image src="/img/Blog_Einfuehrung_3/3_2_Image.jpg"
          alt="Street-Level: YOLO-Punkte zu Heatmaps und Zonal-Statistik je Segment"
          style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </div>

      <div className="sub-detail">
        <strong>Input:</strong>
        <ul>
          <li>YOLO-Detektionen auf Street-Level-Fotos als <b>Punkte</b> (Autos, Busse, Velos, Fussgänger ...)</li>
          <li>Raster-Parameter: Pixelgrösse, Radius, `min_conf`</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Algorithmus:</strong>
        <ul>
          <li>Qualitätsfilter (Nachbarn/Effort)</li>
          <li>Heatmaps je Objektgruppe (Perzentil + Glättung)</li>
          <li>Zonal-Statistik entlang Kanten → `rast_*` je Gruppe</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Output:</strong>
        <ul>
          <li>GeoTIFF-Heatmaps</li>
          <li>`edges_raster.parquet` – Kanten mit `rast_*`</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Bildvorschlag:</strong>
        <ul>
          <li>Heatmap „Autos“ als halbtransparente Fläche</li>
          <li>Strassensegmente mit Tooltip „rast_car = 0.82“</li>
          <li>Beispiel-Street-Level-Foto (klein) als Einblendung</li>
        </ul>
      </div>
    </div>

    <div className="sub-box">
      <div className="sub-title">3.3 Klassifikation mit ML (inkl. Monte Carlo)</div>

      <div className="sub-detail">
        <Image src="/img/Blog_Einfuehrung_3/3_3_Image.jpg"
          alt="ML-Klassifikation: Features → Safety-Score mit Monte-Carlo-Unsicherheit"
          style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </div>

      <div className="sub-detail">
        <strong>Input:</strong>
        <ul>
          <li>Features aus 3.1 und 3.2 (Klassenanteile, `rast_*`, Länge)</li>
          <li>Ground-Truth der Stadtpolizei Zürich (Übergänge mit Schwierigkeit)</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Algorithmus:</strong>
        <ul>
          <li>Preprocessing (Imputation, One-Hot wo nötig, ggf. Klassengewichte)</li>
          <li>Modelle: z. B. RandomForest / HistGradientBoosting, 5-Fold CV</li>
          <li>Vorhersage: `prob_unsafe` → `safety_score = 1 - Risiko`</li>
          <li><b>Monte Carlo:</b> Modell mehrfach mit kleinen Zufallsvariationen laufen lassen → p05/p95 als Unsicherheitsintervall</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Output:</strong>
        <ul>
          <li>`edges_with_safety_ml.parquet` – `safety_score`, `prob_unsafe`, `mc_p05`, `mc_p95`</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Bildvorschlag:</strong>
        <ul>
          <li>Kartenansicht mit Segmentfärbung nach `safety_score`</li>
          <li>Kleines Histogramm/Violinplot der Monte-Carlo-Verteilung für ein Beispielsegment</li>
          <li>Legende: sicher ↔ unsicher</li>
        </ul>
      </div>
    </div>

    <div className="sub-box">
      <div className="sub-title">3.4 Klassifikation regelbasiert (Punktabzug)</div>

      <div className="sub-detail">
        <Image src="/img/Blog_Einfuehrung_3/3_4_Image.jpg"
          alt="Regelbasiertes Scoring: Abzüge/Bonis aus YOLO-Polygonen und Heatmaps"
          style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </div>

      <div className="sub-detail">
        <strong>Input:</strong>
        <ul>
          <li>Features aus 3.1 und 3.2</li>
          <li>Einfache Gewichte und Schwellwerte (Regelset)</li>
          <li>Übergangs-Flag/Kreuzungslogik (markiert, Ampel, Insel)</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Algorithmus:</strong>
        <ul>
          <li>Startscore = 1.0</li>
          <li>Abzüge: z. B. `rast_car` hoch −0.25, Tram/Bus −0.10, *kein* markierter Übergang −0.30</li>
          <li>Bonis: 30er-Zone +0.10, Veloweg +0.08, Zebrastreifen +0.15, Ampel +0.25</li>
          <li>Clipping/Normalisierung → `safety_score_rule ∈ [0,1]`</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Output:</strong>
        <ul>
          <li>`edges_with_safety_rule.parquet` – `safety_score_rule` + Kategorie</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Bildvorschlag:</strong>
        <ul>
          <li>Zwei nebeneinander: Karte „ML“ vs. „Regeln“ für dasselbe Gebiet</li>
          <li>Ein Segment mit Overlay der Faktoren (Badges: 30er-Zone +0.10, kein Zebrastreifen −0.30 …)</li>
          <li>Minilegende der Punkteabzüge/Bonis</li>
        </ul>
      </div>
    </div>

    <div className="sub-box">
      <div className="sub-title">3.5 Routing mit Kostenfunktion & API</div>

      <div className="sub-detail">
        <Image src="/img/Blog_Einfuehrung_3/3_5_Image.jpg"
          alt="Routing: Kosten = Alpha*Länge + Beta*(1-Safety); API mit Start- und Zielpunkt"
          style={{ width: '100%', height: 'auto', maxWidth: '800px' }} width={400} height={250} />
      </div>

      <div className="sub-detail">
        <strong>Input:</strong>
        <ul>
          <li>Kanten + `safety_score` (ML) und/oder `safety_score_rule`</li>
          <li>Start- und Zielpunkt (API-Request)</li>
          <li>Parameter: <b>alpha (α)</b> für Länge, <b>beta (β)</b> für Risiko</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Algorithmus:</strong>
        <ul>
          <li>Kosten je Kante: `cost = α · länge_m + β · (1 - safety)`</li>
          <li>k-alternative Pfade (z. B. Yen) berechnen</li>
          <li>Praxiswerte: `α = 1.0`, `β = 2.0` (Sicherheit doppelt so wichtig wie 100 m)</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>API:</strong>
        <ul>
          <li>Einfaches Endpoint: Input = 2 Punkte (Start, Ziel)</li>
          <li>Output = Route als Liste von Kanten mit `länge_m`, `safety`, `cost`</li>
          <li>Optional: Varianten „kürzester“ vs. „sicherster“ Weg</li>
        </ul>
      </div>

      <div className="sub-detail">
        <strong>Output:</strong>
        <ul>
          <li>GPKG/GeoJSON: `routes_*`, `segments_*`</li>
          <li>Tabelle pro Route: Länge, mittlere Safety, Anzahl Kreuzungen</li>
        </ul>
      </div>

<div className="sub-detail">
  <RevealBox title="Beispiel API-Aufruf">

~~~json
POST /route
{
  "start": [2685000.0, 1248000.0],
  "ziel": [2685600.0, 1248400.0],
  "alpha": 1.0,
  "beta": 2.0
}

Response:
{
  "route_id": "r1",
  "segments": [
    { "id": 101, "länge_m": 85.2, "safety": 0.82, "cost": 0.36 },
    { "id": 102, "länge_m": 47.8, "safety": 0.90, "cost": 0.20 }
  ],
  "summary": { "total_länge": 133.0, "avg_safety": 0.86, "kreuzungen": 1 }
}
~~~

  </RevealBox>
</div>


      <div className="sub-detail">
        <strong>Bildvorschlag:</strong>
        <ul>
          <li>Karte mit zwei hervorgehobenen Routen (blau = kürzester, grün = sicherster)</li>
          <li>Popup je Segment mit `länge`, `safety`, `cost`</li>
          <li>Kleiner Kasten „API-Beispiel“ mit obigem JSON-Snippet</li>
        </ul>
      </div>
    </div>
  </div>
</div>




<div className="main-stage">
    <div className="main-title">4. Visualisierung</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>...</li>
        </ul>
      </>
    </div>

    
    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul>
          <li>...</li>
          <li>...</li>
          <li>...</li>
        </ul>
        <strong>Herausforderungen:</strong>
        <ul>
          <li>...</li>
          <li>...</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">
      <div className="sub-box">
        <div className="sub-title">4.1 QGIS Web Client</div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>

      <div className="sub-box">
        <div className="sub-title">4.3 Website</div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>

    </div>

    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul><li>...</li></ul>
      </>
    </div>
  </div>

</div>

