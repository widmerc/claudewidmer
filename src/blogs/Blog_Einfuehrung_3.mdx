export const metadata = {
  title: "Arbeitsplan Masterarbeit (Stand August)",
  excerpt: "Mein Plan für die Organisation der Masterarbeit",
  coverImage: "/img/Blog_Einfuehrung_3/Titelbild.jpg",
  date: "2025-05-22T12:00:00.000Z",
  author: {
    name: "Claude Widmer",
    picture: "/img/Blog1_2/0.jpg"
  },
  tags: ["Masterarbeit"]
}

# Arbeitsplan Masterarbeit

In diesem Blogbeitrag stelle ich meinen geplanten Arbeitsablauf für die Masterarbeit vor.

Im Folgenden findest du einen visuellen Arbeitsplan mit den wichtigsten Schritten:

<div className="timeline-container">

  <div className="timeline-step">
    <div className="timeline-number">1</div>
    <div className="timeline-box">
      <b>Erarbeitung eines Modells (Raster) für die Modellierung des Raumes</b><br />
      Wo kann man durchlaufen? Erstellung eines Rasters als Grundlage für die weitere Analyse.
    </div>
  </div>

  <div className="timeline-step">
    <div className="timeline-number">2</div>
    <div className="timeline-box">
      <b>Feature-Erkennung & Geolokalisierung</b>
      <ul>
        <li><b>2.0:</b> Mapillary Data Retrieval – Download und Metadaten-Erfassung der Bildkacheln.</li>
        <li><b>2.1:</b> YOLO Image Recognition Training – Training und Evaluation der Modelle.</li>
        <li><b>2.2:</b> Depth Estimation – Berechnung von Tiefenkarten und Ableitung von Distanzklassen.</li>
        <li><b>2.3:</b> YOLO Object Detection Processing – Inferenz auf validen Bildern und Zusammenführen der Ergebnisse.</li>
        <li><b>2.4:</b> Object Geolocation – Projektion der Detektionen in LV95 mittels Kamerawinkel & FOV-Offset.</li>
      </ul>
    </div>
  </div>

  <div className="timeline-step">
    <div className="timeline-number">3</div>
    <div className="timeline-box">
      <b>Klassifikation der Schulwegsicherheit</b><br />
      <ul>
        <li><b>3.1:</b> Bewertung der Sicherheit von Schulwegen anhand der in Schritt&nbsp;2 gewonnenen Objektdaten mit Machine-Learning-Techniken.</li>
        <li><b>3.2:</b> Ergänzende Klassifikation durch regelbasierte Parameter (definierte Kriterien zur Einschätzung der Sicherheit).</li>
      </ul>
      Ziel ist es, die erkannten Objekte systematisch in Bezug auf ihre Relevanz für die
      Schulwegsicherheit auszuwerten und daraus eine nachvollziehbare Einschätzung abzuleiten.
    </div>
  </div>

  <div className="timeline-step">
    <div className="timeline-number">4</div>
    <div className="timeline-box">
      <b>Webkarten-Integration</b><br />
      Aufbau einer interaktiven Karte zur Visualisierung der Resultate 
      (Objektdetektionen, Geolokalisierung, Sicherheitsklassifikation).
    </div>
  </div>
</div>

<hr />

# Arbeitsplan Masterarbeit (Detailansicht)

<div className="multi-stage">

  {/* 1 – Rastermodell */}
  <div className="main-stage">
    <div className="main-title">1. Erarbeitung eines Modells (Raster)</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Bounding Box von Zürich</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">
      <div className="sub-box">
        <div className="sub-title">1.1 Raster-Erstellung</div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>
    </div>

    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul><li>wird ergänzt</li></ul>
      </>
    </div>
  </div>

  {/* 2 – Feature-Erkennung & Geolokalisierung */}
  <div className="main-stage">
    <div className="main-title">2. Feature-Erkennung & Geolokalisierung</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Bounding Box von Zürich</li>
          <li>Mapillary API Token(s)</li>
          <li>Stammverzeichnis (z. B. <code>ROOT_PATH = ./</code>)</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">

      <div className="sub-box">
        <div className="sub-title">2.0 Mapillary Data Retrieval</div>
                
        <div className="sub-detail">
          <>
          <img src="/img/Blog_Einfuehrung_3/2_0_Image.png" alt="Blog Einführung 3 - Bild 2.0" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul>
              <li><code>Bounding Box</code> von Zürich</li>
              <li><code>GPKG_PATH = ./data/images_bbox_fullmeta.gpkg</code></li>
              <li>Spalte <code>thumb_1024_url</code> mit 1024p-URLs</li>
              <li><code>LAPLACIAN_THRESHOLD</code>: Threshold für die Bildunschärfen-Erkennung (100)</li>
            </ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul>
              <li><strong>Laden von Geodaten:</strong> Das Notebook lädt Geodaten und Metadaten über die Mapillary API herunter (über 1,2 Millionen Bilder) und bereitet sie für die Analyse vor.</li>
              <li><strong>Erkennung von unscharfen Bildern:</strong> Es überprüft die Bildqualität, um unscharfe Bilder zu identifizieren via Laplacian Methode (mit einem Threshold).</li>
              <li><strong>Zuordnung der Ergebnisse:</strong> Die Ergebnisse der Unschärfeprüfung werden den Bildern zugeordnet.</li>
              <li><strong>Speichern der Daten:</strong> Die aktualisierten Geodaten werden in einer neuen GeoPackage-Datei gespeichert.</li>
            </ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul>
              <li><strong> Geopackage </strong> mit allen Metadaten von den 1.2 Millionen Bildern und die Information, ob das Bild unscharf ist.</li>
            </ul>
          </>
        </div>
      </div>

<div className="sub-box">
  <div className="sub-title">2.1 YOLO Image Recognition Training</div>
          <div className="sub-detail">
          <>
          <img src="/img/Blog_Einfuehrung_3/2_1_Image.png" alt="Blog Einführung 3 - Bild 2.1" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
          </>
        </div>
  <div className="sub-detail">
    <>
      <strong>Input:</strong>
      <ul>
        <li>Mapillary Segmentation and Object Detection Dataset</li>
        <li>YOLO-kompatible Labeldateien (YOLOv8 Format)</li>
      </ul>
    </>
  </div>
  <div className="sub-detail">
    <>
      <strong>Algorithmus:</strong>
      <ul>
        <li>Laden von Trainings- und Validierungsdaten aus YOLO-kompatiblen Pfaden</li>
        <li>Verwendung eines YOLOv8-Medium-Modells (<code>yolo11m-seg.pt</code>) mit vortrainierten Gewichten</li>
        <li>Durchführung eines Segmentierungs- und Objekterkennungs-Trainings mit Mixed Precision (AMP)</li>
        <li>Anwendung von Transfer Learning und Feinjustierung über 100+ Epochen mit Early Stopping</li>
        <li>Optimizer automatisch gewählt (AdamW) mit automatischer Lernratenanpassung</li>
        <li>Evaluation nach jeder Epoche mit mAP, Precision und Recall</li>
        <li>Training und Vergleich unterschiedlicher Modellgrössen (n, s, m, l)</li>
        <li>Über 12 Stunden Trainingszeit für das Medium-Modell, insgesamt über 48 Stunden zur Modellevaluierung (RTX 4080, 64 GB RAM)</li>
      </ul>
    </>
  </div>
  <div className="sub-detail">
    <>
      <strong>Output:</strong>
      <ul>
        <li>Trainiertes YOLO-Modell (<code>best.pt</code>)</li>
        <li>Evaluationsmetriken (Precision, Recall, mAP50, mAP50-95)</li>
        <li>Trainingsplots und Ergebnisvisualisierungen</li>
      </ul>
    </>
  </div>
<div className="sub-detail">
      <>
        <strong>Probleme:</strong>
        <ul>
          <li>Aufgrund des sehr grossen Datensatzes (über 18'000 Bilder) konnte nicht der gesamte Umfang für das Training genutzt werden.</li>
          <li>Die verfügbare Rechenleistung (GPU/VRAM) war limitiert, was zu kleineren Batchgrössen und kürzeren Trainingszeiten führte.</li>
          <li>Dadurch litt insbesondere der <strong>Recall</strong> (Erkennungsrate), da das Modell viele Objekte nicht zuverlässig detektierte.</li>
          <li>Zusätzlich erschwerten die grosse Klassenvielfalt und unbalancierte Verteilung die Generalisierung.</li>
        </ul>
      </>
    </div>
</div>

  <div className="sub-box">
    <div className="sub-title">2.2 Depth Estimation Processing</div>
    <div className="sub-detail">
      <>
      <img src="/img/Blog_Einfuehrung_3/2_2_Image.png" alt="Blog Einführung 3 - Bild 2.2" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Die 1.2 Millionen Bilder vom Schritt 2.0</li>
          <li>Pfadliste aller validen Bilder (vorvalidiert via PIL)</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Algorithmus:</strong>
        <ul>
          <li>Scannen aller .jpg-Dateien im Datensatzordner</li>
          <li>Parallelvalidierung mittels PIL, um defekte Bilder zu filtern</li>
          <li>Reduktion der Bildauflösung (50 %) zur Performance-Optimierung</li>
          <li>Batchweise Tiefenschätzung mittels <code>depth-anything</code> Pipeline (HuggingFace)</li>
          <li>Optionales Invertieren der Tiefenwerte für GIS-Konsistenz</li>
          <li>Export der Tiefendaten als .npz (komprimiert, float16)</li>
          <li>Optional: Export von Tiefenbildern und Histogrammen zur visuellen Kontrolle</li>
          <li>Speicherbereinigung und GPU-Freigabe nach jeder Batch für stabile Laufzeit</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Tiefenkarten im .npz-Format (komprimiert, float16)</li>
          <li>Optional: Visualisierungen der Tiefenbilder (.png)</li>
          <li>Optional: Histogramme der Tiefenverteilung pro Bild</li>
          <li>Gespeichert im Unterordner <code>depth_processed/</code></li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Probleme:</strong>
        <ul>
          <li>Volle Auflösung führte zu Out-of-Memory-Fehlern → Downscaling nötig</li>
          <li>Einige Bilder konnten trotz vorheriger Validierung nicht verarbeitet werden (Pipeline-Fehler)</li>
          <li>Speicherfreigabe und Garbage Collection war nötig, um GPU-Nutzung stabil zu halten</li>
          <li>Laufzeit von über 12 Stunden für alle Bilder (RTX 4080, 64 GB RAM, 128er Batch-Grösse)</li>
          <li>Hohe Modellqualität, aber keine Echtzeitverarbeitung möglich</li>
        </ul>
      </>
    </div>
  </div>


  <div className="sub-box">
    <div className="sub-title">2.3 YOLO Object Detection Processing</div>
    <div className="sub-detail">
      <>
      <img src="/img/Blog_Einfuehrung_3/2_3_Image.jpg" alt="Blog Einführung 3 - Bild 2.3" style={{ width: '100%', height: 'auto', maxWidth: '400px' }} />
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Input:</strong>
        <ul>
          <li>Gefilterte Bildliste aus Schritt 2.2 (inkl. validem Tiefendatenpfad)</li>
          <li>Vortrainiertes YOLO-Modell (<code>.pt</code>)</li>
          <li>Konfigurationsdatei mit Schwellenwerten, Pfaden und Parametern</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Algorithmus:</strong>
        <ul>
          <li>Filterung aller Bilder ohne Tiefenkarte oder bei Unschaerfe</li>
          <li>Batchweises Kopieren der Bilder in temporären Inferenzordner</li>
          <li>Ausführung der YOLO-Inferenz (<code>ultralytics.YOLO</code>) in Batches</li>
          <li>Speichern der Ergebnisse als <code>.npz</code> pro Bild (BBox, Klassen, Konfidenzen, Maske)</li>
          <li>Zusammenführung von Tiefen- und YOLO-Daten pro Objekt (inkl. Tiefenwertbestimmung)</li>
          <li>Kategorisierung der Objekte anhand der Tiefe (z.B. "near", "medium", "far")</li>
          <li>Optional: Visualisierung der Objekte mit Tiefenklassen</li>
          <li>Speichern aller Objekte in <code>.parquet</code>-Datei (schnell + komprimiert)</li>
          <li>Geometrie-Zuordnung via <code>GeoPackage</code> & Polars für performantes Join</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Output:</strong>
        <ul>
          <li>YOLO-Ergebnisse als komprimierte <code>.npz</code>-Dateien</li>
          <li>Vereinte Objektliste mit Tiefe, Klasse, Konfidenz und Koordinaten (<code>.parquet</code>)</li>
          <li>Optional: Visualisierte Objekte mit Bounding Boxes und Tiefenklassen (.jpg)</li>
          <li>Finale Objekt-Geodaten mit X/Y-Koordinaten aus GPKG</li>
        </ul>
      </>
    </div>
    <div className="sub-detail">
      <>
        <strong>Probleme:</strong>
        <ul>
          <li>Inferenzzeit über 30 Stunden bei 1.2 Mio Bildern → kein Echtzeitbetrieb</li>
          <li>Fehlerhafte oder fehlende <code>.npz</code>-Dateien mussten manuell gefiltert werden</li>
          <li>Initial hohe RAM/GPU-Auslastung – Speicherbereinigung und batchweises Laden notwendig</li>
          <li>Konvertierung von GPKG zu Parquet nötig für effizientes Polars-Join</li>
          <li>Batch-Grösse musste angepasst werden, um stabile Performance auf RTX 4080 zu erreichen</li>
        </ul>
      </>
    </div>
  </div>

<div className="sub-box">
  <div className="sub-title">2.4 Object Geolocation</div>

  <div className="sub-detail">
    <img 
      src="/img/Blog_Einfuehrung_3/2_4_Image.jpg" 
      alt="Objekt-Geolokalisierung: von Bild-Detektion zu LV95-Koordinaten" 
      style={{ width: '100%', height: 'auto', maxWidth: '400px' }} 
    />
  </div>

  <div className="sub-detail">
    <strong>Input:</strong>
    <ul>
      <li>YOLO-Detektionen als <code>.parquet</code> (BBox, Klasse, Konfidenz, z_class)</li>
      <li>Mapillary GPKG mit Kameraposition und Metadaten (z. B. <code>id</code>, <code>is_pano</code>, <code>computed_compass_angle</code>, <code>computed_geometry</code>)</li>
      <li>Parameter: Bildbreite (<code>image_width</code>) und Sichtfeld (<code>fov</code>)</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Algorithmus (ohne OSM):</strong>
    <ul>
      <li>
        Join YOLO ↔ Mapillary:
        <ul>
          <li>Match über <code>image_id</code> (YOLO) und <code>id</code> (Mapillary); Filtere irrelevante Labels und sehr ferne z-Klassen.</li>
          <li>Erzeuge Geometrie aus <code>computed_geometry</code> (WGS84) und projiziere nach LV95 (<code>EPSG:2056</code>).</li>
        </ul>
      </li>
      <li>
        Winkel- und Distanz-Adjustierung pro Objekt:
        <ul>
          <li>Bestimme BBox-Zentrum (<code>cx</code>) und berechne Blickrichtungs-Offset: <code>((cx / image_width) − 0.5) * fov</code>.</li>
          <li>Finaler Azimut = <code>computed_compass_angle + Offset</code>.</li>
          <li>Verschiebe den Kamera-Punkt entlang dieses Winkels um eine z-klassenabhängige Distanz (z. B. <b>very near: 5 m</b>, <b>near: 7.5 m</b>, <b>medium: 20 m</b>).</li>
        </ul>
      </li>
      <li>Parallelisierung in Chunks, Schreiben als <code>.parquet</code> (LV95-Punkte mit <code>x_adj</code>/<code>y_adj</code>).</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Output:</strong>
    <ul>
      <li><code>joined_dataset.parquet</code> – verknüpfte Rohdaten (Kamerapositionen)</li>
      <li><code>adjusted_dataset.parquet</code> – geolokalisierte Objektpunkte (LV95) mit Adjustierung</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Herausforderungen:</strong>
    <ul>
      <li>Approximative Distanzen aus z-Klassen → Positionsfehler abhängig von FOV und BBox-Lage</li>
      <li>Unterschiedliche Bildtypen (Panorama vs. Nicht-Panorama)</li>
      <li>Performance bei sehr grossen Datensätzen → Multiprocessing, Parquet einsetzen</li>
    </ul>
  </div>
</div>





<div className="sub-box">
  <div className="sub-title">2.5 YOLO Objekterkennung auf SWISSIMAGE Orthofotos</div>
  <div className="sub-detail">
    <img 
      src="/img/Blog_Einfuehrung_3/2_5_Image.jpg" 
      alt="Blog Einführung 3 - Bild 2.5" 
      style={{ width: '100%', height: 'auto', maxWidth: '400px' }} 
    />
  </div>

  <div className="sub-detail">
    <strong>Input:</strong>
    <ul>
      <li>SWISSIMAGE Orthofotos von Zürich (170 km² ≈ 120'000 px × 140'000 px)</li>
      <li>8'000 manuell digitalisierte Trainingsfeatures für das YOLO-Modell</li>
      <li>OSM-Daten für Tramlinien und Tramnetz im Kanton Zürich</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Algorithmus:</strong>
    <ul>
      <li>
        Entwicklung von vier YOLO-Modellen:
        <ul>
          <li>Modell 1: YOLOv11L – 100 Epochs – Klassen: 30er-Zone, Auto, Fussgängerstreifen, Pfeilmarkierung, Schule, Tram, Vortritt und Zug</li>
          <li>Modell 2: YOLOv11X – 70 Epochs – Fokus Tram im Kanton Zürich</li>
          <li>Modell 3: YOLOv11X – 100 Epochs – Fokus Strassenmarkierungen in Luzern</li>
          <li>Modell 4: YOLOv11L – 100 Epochs – Fokus Tram in der Stadt Bern (mit OSM-Daten)</li>
        </ul>
      </li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Output:</strong>
    <ul>
      <li>YOLO-Ergebnisse als <code>.gpkg</code>-Dateien</li>
    </ul>
  </div>

  <div className="sub-detail">
    <strong>Herausforderungen:</strong>
    <ul>
      <li>YOLO-Training dauerte über 6 Stunden</li>
      <li>Interferenzzeit aller vier Modelle inkl. Zusammenführung: mehr als 11.5 Stunden</li>
      <li>Hohe RAM/GPU-Auslastung – viel Ausprobieren nötig für optimale Batchgrösse, Bildgrösse und Modellwahl</li>
      <li>Allgemeine Big-Data-Probleme (alles wurde lokal auf dem PC berechnet)</li>
    </ul>
  </div>
</div>



  </div>
    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul>
          <li>Geopackage-Datei mit Metadaten zu über 1.2 Millionen Bildern (inkl. Unschärfe-Information)</li>
          <li>Trainiertes YOLO-Modell zur Objekterkennung (<code>best.pt</code>)</li>
          <li>Tiefenkarten zu validen Bildern im <code>.npz</code>-Format (komprimiert, float16)</li>
          <li>Gefilterte Liste valider Bilder mit vorhandener Tiefenkarte und guter Bildqualität</li>
          <li>YOLO-Erkennungsergebnisse pro Bild als <code>.npz</code>-Dateien (BBox, Klassen, Konfidenzen)</li>
          <li>Kombinierte Objektdaten mit Tiefe und Kategorisierung ("near", "medium", "far") als <code>.parquet</code></li>
          <li>Finale Objektliste mit zugeordneten Koordinaten (via Geometrie-Join) für Mapping-Tools</li>
          <li>Optional: Visualisierte Objekte mit Tiefenklassifikation (.jpg) für Qualitätsprüfung</li>
        </ul>
      </>
    </div>

    <div className="main-output">
      <>
        <strong>Probleme & Aufwand:</strong>
        <ul>
          <li>Extrem hoher Rechenaufwand: Die gesamte Verarbeitung von Bild-Download, Tiefenschätzung und YOLO-Inferenz dauerte mehrere **Tage**, trotz RTX 4080 und 64 GB RAM.</li>
          <li>Der Schritt 2.2 (Tiefenschätzung) benötigte über **12 Stunden** allein für die validen Bilder.</li>
          <li>Die Inferenz mit YOLO in Schritt 2.3 dauerte über **30 Stunden**, was Echtzeitverarbeitung ausschliesst.</li>
          <li>Hohes Risiko von Out-of-Memory-Fehlern, insbesondere bei voller Auflösung → Reduktion der Bildgrösse war zwingend.</li>
          <li>Manuelle Filterung notwendig bei fehlerhaften oder leeren <code>.npz</code>-Ergebnissen</li>
          <li>Speicherfreigabe und Garbage Collection mussten explizit eingebaut werden, um GPU-Laufzeit zu stabilisieren.</li>
          <li>Join von Geometrien auf über 1 Million Objekte erforderte Konvertierung von GPKG zu Parquet und Einsatz von Polars für Performance.</li>
        </ul>
        <p><i>Fazit:</i> Die Datenverarbeitung in Phase 2 war technisch erfolgreich, aber extrem zeit- und ressourcenintensiv. Ich bin sehr froh, hat es geklappt.</p>
      </>
    </div>
  </div>


<div className="main-stage">
    <div className="main-title">3. Klassifikation der Schulwegsicherheit</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>...</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">
      <div className="sub-box">
        <div className="sub-title">3.1 </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>

      <div className="sub-box">
        <div className="sub-title">3.2 </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>

      <div className="sub-box">
        <div className="sub-title">3.3 </div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>

    </div>

    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul><li>wird ergänzt</li></ul>
      </>
    </div>
  </div>


<div className="main-stage">
    <div className="main-title">4. Visualisierung</div>
    <div className="main-input">
      <>
        <strong>Input:</strong>
        <ul>
          <li>...</li>
        </ul>
      </>
    </div>

    <div className="sub-steps">
      <div className="sub-box">
        <div className="sub-title">4.1 QGIS Web Client</div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>

      <div className="sub-box">
        <div className="sub-title">4.3 Website</div>
        <div className="sub-detail">
          <>
            <strong>Input:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Algorithmus:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
        <div className="sub-detail">
          <>
            <strong>Output:</strong>
            <ul><li>wird ergänzt</li></ul>
          </>
        </div>
      </div>

    </div>

    <div className="main-output">
      <>
        <strong>Output:</strong>
        <ul><li>wird ergänzt</li></ul>
      </>
    </div>
  </div>

</div>

