Der Unterschied zwischen den Multiprocessing und dem Single Processing vom 

from concurrent.futures import ThreadPoolExecutor, as_completed


if mode == "parallel":

	with ThreadPoolExecutor() as executor:
		futures = [executor.submit(process_image, image, output_folder, remove_blurry_images) for image in images]
		for f in as_completed(futures):
			result = f.result()
		if result is not None:
			all_data.append(result)
else:  # single processing
	for image in images:
		result = process_image(image, output_folder, remove_blurry_images)
		if result is not None:
			all_data.append(result)



2025-06-20 15:49:16,246 - INFO - Mode: single | Images: 98 | Time: 66.95 s
2025-06-20 15:49:28,574 - INFO - Mode: parallel | Images: 100 | Time: 12.33 s
-> 5x so schnell



Allgemein:

if __name__ == "__main__":
    output_folder = r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\Exports\Geolocation\input" <- Output für die Bilder / JSON / GPKG
    geojson_path = r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\Scripts\2_Feature_Geolocation\bbox.geojson" <- für input
    max_images = None <- unlimitiert viele downloaden
    use_computed_geometry = True <- Mapillary
    main(
        geojson_path=geojson_path,
        output_folder=output_folder,
        use_computed_geometry=use_computed_geometry,
        max_images=max_images,
        remove_blurry_images=True
    )



>Mein Problem:<

-> Mapillary hat max. 100 Punkte / Bilder, welches sie in ihre API gibt. Das heisst, man muss irgendwie einen kleveren Algorithmus machen, dass es geht.

1) Idee 1:
-> Multiprocessing einfach mit einem Grid von 10 Metern und dann ganz schnell alle durchprobieren. -> geht nicht, hat immer API ausgelastet und Probleme bereitet

2) Idee 2:
-> X Worker mit einem eigenen Access key durchgehen. Funktioniert, ist aber Ultra langsam wenn man über 900'000 Kacheln durchgehen muss. Pro Kachel ca. 0.3 Sekunden

3) Idee 3:
-> Wie WMS Pyramide. Ich schaue, wo es Punkte hat über die API über ein grösseres Grid, und wenn ja, dann gehe ich das grid durch, und wenn nein, dann schreibe ich alle darin kleineren Grid-teile schon ihr .json. (-> wenn es abstürzen würde, dann weiss ich welche ich schon durchgegangen bin)

1 km x 1 km -> wenn ja, dann 500m x 500m, wenn ja, dann 250m x 250m … bis ich die 10m erreicht habe. Das geht schneller, und die threshholds kann ich so definieren wie ich möchte. Wenn nichts gefunden worden ist, dann kann ich mit einem get-feature schauen welche 10m x 10m betroffen sind und die corrensponding .json schreiben -> [] also leere json. Und pass, wenn schon in einem grösseren tile schon die fine_tiles durchgegangen worden sind.
-> EWIGKEITEN

4) Load über Vector Tiles und dann hat man image_id -> dann über die Mapillary https://graph.mapillary.com/{image_id}?access_token={ACCESS_TOKEN}&fields={fields_str} kann man dann für jedes bild noch die Tabelle erweitern. 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------

"""
On importing the interface
"""

import mapillary.interface as mly
from config import ACCESS_TOKEN
import mapillary.config as mly_config # Importing desired configurations
import mapillary.controller as mly_controller # Importing desired controllers
import mapillary.models as mly_models # Importing the used models
import mapillary.utils as mly_utils # Importing utility material
import requests
import json
import shapely.geometry

# Setze deinen Mapillary Access Token
mly.set_access_token(ACCESS_TOKEN)


bbox = [[8.457720822843683, 47.390649130286448], [8.456825005154048, 47.37424440909907], [8.513957476598119, 47.334770736179166], [8.538369416730317, 47.347789816857421], [8.566987407296029, 47.345832805990639], [8.578446117577144, 47.351059452649729], [8.615089558171336, 47.363347841537582], [8.593951036370918, 47.38175404740641], [8.598868228275775, 47.407016330860856], [8.574228138587589, 47.412600163192216], [8.55660690453244, 47.419692963652167], [8.556685022028232, 47.437291328914533], [8.514688737203723, 47.435910038955718], [8.476965065062107, 47.42933398525328], [8.466429431361696, 47.419442231388643], [8.457720822843683, 47.390649130286448]]

# Berechne Bounding Box (min/max für west, south, east, north)
lons = [pt[0] for pt in bbox]
lats = [pt[1] for pt in bbox]
bbox_dict = {
    "west": min(lons),
    "south": min(lats),
    "east": max(lons),
    "north": max(lats),
}

data = json.loads(
    mly.images_in_bbox(bbox_dict)
)

# convert this .json to a geodataframe file
import geopandas as gpd
def json_to_geodataframe(json_data):
    """
    Convert JSON data to a GeoDataFrame.

    :param json_data: JSON data containing features with geometry
    :type json_data: dict
    :return: GeoDataFrame containing the features
    :rtype: gpd.GeoDataFrame
    """
    features = json_data.get("features", [])
    geometries = []
    properties = []
    for feature in features:
        geometry = feature.get("geometry")
        properties.append(feature.get("properties", {}))
        if geometry:
            if geometry["type"] == "Point":
                geometries.append(shapely.geometry.Point(geometry["coordinates"]))
            elif geometry["type"] == "LineString":
                geometries.append(shapely.geometry.LineString(geometry["coordinates"]))
            elif geometry["type"] == "Polygon":
                geometries.append(shapely.geometry.Polygon(geometry["coordinates"][0]))
    gdf = gpd.GeoDataFrame(properties, geometry=geometries, crs="EPSG:4326")
    return gdf

gdf = json_to_geodataframe(data)

gdf.to_file(r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\images_bbox.gpkg", driver='GPKG')



from config import ACCESS_TOKEN
import mapillary.config as mly_config # Importing desired configurations
import mapillary.controller as mly_controller # Importing desired controllers
import mapillary.models as mly_models # Importing the used models
import mapillary.utils as mly_utils # Importing utility material
import requests
import json
import shapely.geometry
import geopandas as gpd
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from tqdm import tqdm
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Lade die ersten 1000 Bilder aus dem GPKG
input_gpkg = r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\images_bbox.gpkg"
gdf = gpd.read_file(input_gpkg)
gdf.crs = "EPSG:4326"

fields = [
    "id",
    "altitude",
    "atomic_scale",
    "camera_parameters",
    "camera_type",
    "captured_at",
    "compass_angle",
    "computed_altitude",
    "computed_compass_angle",
    "computed_geometry",
    "computed_rotation",
    "creator",
    "exif_orientation",
    "geometry",
    "height",
    "make",
    "model",
    "thumb_256_url",
    "thumb_1024_url",
    "thumb_2048_url",
    "thumb_original_url",
    "merge_cc",
    "mesh",
    "sfm_cluster",
    "width",
    "detections"
]
fields_str = ",".join(fields)

ACCESS_TOKEN = ACCESS_TOKEN


def fetch_metadata(image_id, session, max_retries=3):
    url = f"https://graph.mapillary.com/{image_id}?access_token={ACCESS_TOKEN}&fields={fields_str}"
    for attempt in range(max_retries):
        try:
            r = session.get(url, timeout=10)
            if r.status_code == 200:
                return r.json()
            else:
                print(f"Fehler bei {image_id}: Status {r.status_code}, Antwort: {r.text}")
        except Exception as e:
            print(f"Fehler bei {image_id} (Versuch {attempt+1}/{max_retries}): {e}")
        time.sleep(2 * (attempt + 1))  # Exponentielles Backoff
    return {}

image_ids = gdf['id'].astype(str).tolist()

results = []
failed_ids = []
max_workers = 10000
print(f"Starte Download mit {max_workers} parallelen Threads für {len(image_ids)} Bilder...")
start_time = time.time()

# Session mit Retry-Adapter für alle Threads
session = requests.Session()
retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retries)
session.mount('https://', adapter)

with ThreadPoolExecutor(max_workers=max_workers) as executor:
    future_to_id = {executor.submit(fetch_metadata, img_id, session): img_id for img_id in image_ids}
    for i, future in enumerate(tqdm(as_completed(future_to_id), total=len(image_ids), desc="API-Requests")):
        data = future.result()
        if data:
            results.append(data)
        else:
            failed_ids.append(future_to_id[future])
        if (i+1) % 50 == 0 or (i+1) == len(image_ids):
            elapsed = time.time() - start_time

if failed_ids:
    print(f"{len(failed_ids)} IDs konnten nicht geladen werden. Siehe failed_ids.txt")
    with open("failed_ids.txt", "w") as f:
        for fid in failed_ids:
            f.write(f"{fid}\n")

# Metadaten als DataFrame
meta_df = pd.DataFrame(results)

# Debug: Zeige die ersten Zeilen und Spaltennamen
print('meta_df.head():')
print(meta_df.head())
print('meta_df.columns:')
print(meta_df.columns)

# Filtere nur Zeilen mit gültiger 'id'
if 'id' in meta_df.columns:
    meta_df = meta_df[meta_df['id'].notnull()]
else:
    print('Warnung: Keine id-Spalte in den API-Ergebnissen!')

# Stelle sicher, dass beide 'id'-Spalten String sind
if 'id' in gdf.columns:
    gdf['id'] = gdf['id'].astype(str)
if 'id' in meta_df.columns:
    meta_df['id'] = meta_df['id'].astype(str)

# Merge mit GeoDataFrame, nur wenn id vorhanden
if not meta_df.empty and 'id' in meta_df.columns:
    merged = gdf.merge(meta_df, on='id', how='left')
    # Speichern
    merged.to_file(r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\images_bbox_fullmeta.gpkg", driver="GPKG")
else:
    print('Kein Merge möglich, da keine gültigen Metadaten vorhanden.')


"""
On importing the interface
"""

import mapillary.interface as mly
from config import ACCESS_TOKEN
import mapillary.config as mly_config # Importing desired configurations
import mapillary.controller as mly_controller # Importing desired controllers
import mapillary.models as mly_models # Importing the used models
import mapillary.utils as mly_utils # Importing utility material
import requests
import json
import shapely.geometry

# Setze deinen Mapillary Access Token
mly.set_access_token(ACCESS_TOKEN)


bbox = [[8.457720822843683, 47.390649130286448], [8.456825005154048, 47.37424440909907], [8.513957476598119, 47.334770736179166], [8.538369416730317, 47.347789816857421], [8.566987407296029, 47.345832805990639], [8.578446117577144, 47.351059452649729], [8.615089558171336, 47.363347841537582], [8.593951036370918, 47.38175404740641], [8.598868228275775, 47.407016330860856], [8.574228138587589, 47.412600163192216], [8.55660690453244, 47.419692963652167], [8.556685022028232, 47.437291328914533], [8.514688737203723, 47.435910038955718], [8.476965065062107, 47.42933398525328], [8.466429431361696, 47.419442231388643], [8.457720822843683, 47.390649130286448]]

# Berechne Bounding Box (min/max für west, south, east, north)
lons = [pt[0] for pt in bbox]
lats = [pt[1] for pt in bbox]
bbox_dict = {
    "west": min(lons),
    "south": min(lats),
    "east": max(lons),
    "north": max(lats),
}

data = json.loads(
    mly.images_in_bbox(bbox_dict)
)

# convert this .json to a geodataframe file
import geopandas as gpd
def json_to_geodataframe(json_data):
    """
    Convert JSON data to a GeoDataFrame.

    :param json_data: JSON data containing features with geometry
    :type json_data: dict
    :return: GeoDataFrame containing the features
    :rtype: gpd.GeoDataFrame
    """
    features = json_data.get("features", [])
    geometries = []
    properties = []
    for feature in features:
        geometry = feature.get("geometry")
        properties.append(feature.get("properties", {}))
        if geometry:
            if geometry["type"] == "Point":
                geometries.append(shapely.geometry.Point(geometry["coordinates"]))
            elif geometry["type"] == "LineString":
                geometries.append(shapely.geometry.LineString(geometry["coordinates"]))
            elif geometry["type"] == "Polygon":
                geometries.append(shapely.geometry.Polygon(geometry["coordinates"][0]))
    gdf = gpd.GeoDataFrame(properties, geometry=geometries, crs="EPSG:4326")
    return gdf

gdf = json_to_geodataframe(data)

gdf.to_file(r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\images_bbox.gpkg", driver='GPKG')



from config import ACCESS_TOKEN
import mapillary.config as mly_config # Importing desired configurations
import mapillary.controller as mly_controller # Importing desired controllers
import mapillary.models as mly_models # Importing the used models
import mapillary.utils as mly_utils # Importing utility material
import requests
import json
import shapely.geometry
import geopandas as gpd
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from tqdm import tqdm
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Lade die ersten 1000 Bilder aus dem GPKG
input_gpkg = r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\images_bbox.gpkg"
gdf = gpd.read_file(input_gpkg)
gdf.crs = "EPSG:4326"

fields = [
    "id",
    "altitude",
    "atomic_scale",
    "camera_parameters",
    "camera_type",
    "captured_at",
    "compass_angle",
    "computed_altitude",
    "computed_compass_angle",
    "computed_geometry",
    "computed_rotation",
    "creator",
    "exif_orientation",
    "geometry",
    "height",
    "make",
    "model",
    "thumb_256_url",
    "thumb_1024_url",
    "thumb_2048_url",
    "thumb_original_url",
    "merge_cc",
    "mesh",
    "sfm_cluster",
    "width",
    "detections"
]
fields_str = ",".join(fields)

ACCESS_TOKEN = ACCESS_TOKEN


def fetch_metadata(image_id, session, max_retries=3):
    url = f"https://graph.mapillary.com/{image_id}?access_token={ACCESS_TOKEN}&fields={fields_str}"
    for attempt in range(max_retries):
        try:
            r = session.get(url, timeout=10)
            if r.status_code == 200:
                return r.json()
            else:
                print(f"Fehler bei {image_id}: Status {r.status_code}, Antwort: {r.text}")
        except Exception as e:
            print(f"Fehler bei {image_id} (Versuch {attempt+1}/{max_retries}): {e}")
        time.sleep(2 * (attempt + 1))  # Exponentielles Backoff
    return {}

image_ids = gdf['id'].astype(str).tolist()

results = []
failed_ids = []
max_workers = 10000
print(f"Starte Download mit {max_workers} parallelen Threads für {len(image_ids)} Bilder...")
start_time = time.time()

# Session mit Retry-Adapter für alle Threads
session = requests.Session()
retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retries)
session.mount('https://', adapter)

with ThreadPoolExecutor(max_workers=max_workers) as executor:
    future_to_id = {executor.submit(fetch_metadata, img_id, session): img_id for img_id in image_ids}
    for i, future in enumerate(tqdm(as_completed(future_to_id), total=len(image_ids), desc="API-Requests")):
        data = future.result()
        if data:
            results.append(data)
        else:
            failed_ids.append(future_to_id[future])
        if (i+1) % 50 == 0 or (i+1) == len(image_ids):
            elapsed = time.time() - start_time

if failed_ids:
    print(f"{len(failed_ids)} IDs konnten nicht geladen werden. Siehe failed_ids.txt")
    with open("failed_ids.txt", "w") as f:
        for fid in failed_ids:
            f.write(f"{fid}\n")

# Metadaten als DataFrame
meta_df = pd.DataFrame(results)

# Debug: Zeige die ersten Zeilen und Spaltennamen
print('meta_df.head():')
print(meta_df.head())
print('meta_df.columns:')
print(meta_df.columns)

# Filtere nur Zeilen mit gültiger 'id'
if 'id' in meta_df.columns:
    meta_df = meta_df[meta_df['id'].notnull()]
else:
    print('Warnung: Keine id-Spalte in den API-Ergebnissen!')

# Stelle sicher, dass beide 'id'-Spalten String sind
if 'id' in gdf.columns:
    gdf['id'] = gdf['id'].astype(str)
if 'id' in meta_df.columns:
    meta_df['id'] = meta_df['id'].astype(str)

# Merge mit GeoDataFrame, nur wenn id vorhanden
if not meta_df.empty and 'id' in meta_df.columns:
    merged = gdf.merge(meta_df, on='id', how='left')
    # Speichern
    merged.to_file(r"C:\Users\claud\Documents\Studium\Masterarbeit\03_Model\images_bbox_fullmeta.gpkg", driver="GPKG")
else:
    print('Kein Merge möglich, da keine gültigen Metadaten vorhanden.')

------------------------------------------------------------------------------------------------------------------------------------------------------------------------


5) 4) gab ein error, weil es zu viel pro minute abfragte: jetzt noch das integriert:

requests_per_minute = 50000  # Einstellbar: Maximale Requests pro Minute
safety_factor = 0.9  # 90% des Limits nutzen, um Schwankungen abzufangen
min_delay = 60.0 / requests_per_minute  # Sekunden Pause pro Request
# Korrekte Berechnung: max_workers * (1/min_delay) <= requests_per_minute * safety_factor
max_workers = max(1, int(requests_per_minute * safety_factor * min_delay))

-> es ging 2h 8min wegen API nicht so schnell. 



